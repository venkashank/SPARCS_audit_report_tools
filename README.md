# SPARCS Report Processing Tools

This project provides a suite of Python scripts designed to download and process various System for Patient Access, Reporting, and Inquiry System (SPARCS) reports from the New York State Department of Health website.

## Scripts Overview

The `src/` directory contains the following scripts:

1.  **`src/compliance_report_puller.py`**
    *   **Functionality:** Downloads PDF files of SPARCS *compliance reports*.
    *   **Source URL:** [https://www.health.ny.gov/statistics/sparcs/reports/compliance/pfi_facilities.htm](https://www.health.ny.gov/statistics/sparcs/reports/compliance/pfi_facilities.htm)
    *   **Output:** Saves the downloaded PDF files into the `pdfs/` directory.

2.  **`src/compliance_table_extractor.py`**
    *   **Functionality:** Processes the PDF compliance reports downloaded by `compliance_report_puller.py` (located in the `pdfs/` directory). It uses the `camelot-py` library to extract tabular data from these PDFs, performs data cleaning and consolidation.
    *   **Output:** Generates a single, consolidated CSV file named `SPARCS_Compliance_Report.csv`.

3.  **`src/audit_report_table_extractor.py`**
    *   **Functionality:** Scrapes *HTML tables* from various web pages listing SPARCS audit reports. These are typically found on sub-pages linked from the main SPARCS reports page (e.g., sections within `https://www.health.ny.gov/statistics/sparcs/reports/`).
    *   **Output:** Saves each extracted HTML table as an individual CSV file into the `output/` directory.
    *   **Note:** The name `audit_report_table_extractor.py` might be slightly misleading as it processes HTML tables from web pages, not PDF audit reports directly in the same way the compliance scripts handle PDFs. A name like `audit_html_table_scraper.py` or `sparcs_html_table_extractor.py` could be more descriptive of its specific function.

## Usage Overview

To run the full data processing pipeline, execute the `main.py` script from the **project root directory** (the directory containing the `src` folder) using one of the following commands:

1.  **Using the module execution flag (recommended):**
    ```bash
    python -m src.main
    ```
2.  **Directly executing the script:**
    ```bash
    python src/main.py
    ```

Both commands will execute all steps in sequence:

1.  Download compliance PDF reports (using logic from `compliance_report_puller.py`).
2.  Extract and process tables from these compliance PDFs (using logic from `compliance_table_extractor.py`).
3.  Process audit report HTML tables (using logic from `audit_report_table_extractor.py`).

The pipeline is designed to stop if any critical step fails, and errors will be logged.

### Running Individual Steps

While `src/main.py` is recommended for a full pipeline run, individual scripts can still be executed independently if only a specific part of the pipeline is needed. Ensure you are in the `src` directory or adjust paths accordingly:

-   To only download compliance PDFs:
    ```bash
    python compliance_report_puller.py
    ```
-   To only process downloaded compliance PDFs (assuming they are already in the `pdfs/` directory):
    ```bash
    python compliance_table_extractor.py
    ```
-   To only process audit report HTML tables:
    ```bash
    python audit_report_table_extractor.py
    ```

## Output Files

The key output files generated by these scripts are:

*   **`SPARCS_Compliance_Report.csv`**: A consolidated CSV containing data extracted from all processed PDF compliance reports.
*   **`pdfs/` (directory)**: Contains the downloaded PDF compliance reports (e.g., `2022_PFI_123456.pdf`).
*   **`output/` (directory)**: Contains individual CSV files, each corresponding to an HTML table scraped by `audit_report_table_extractor.py` (e.g., `audit_data_export_2023.csv`).